{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31026c5f-6a1b-4d88-9d72-cbd3f3d800ee",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226de73d-0f5d-4f5c-830b-ae78dd905e12",
   "metadata": {},
   "source": [
    "## Topic 1: Concept ‚Üí Bagging & Ensemble Learning\n",
    "## Part 1: Ensemble Learning vs Bagging\n",
    "### 1. Ensemble Learning (general idea)\n",
    "\n",
    "Think of Ensemble as: ‚ÄúDon‚Äôt trust one model, combine many.‚Äù\n",
    "\n",
    "It‚Äôs like asking multiple people instead of one person.\n",
    "\n",
    "Techniques: Bagging, Boosting, Stacking (all are types of ensemble learning).\n",
    "\n",
    "üëâ Example:\n",
    "Election Prediction üó≥Ô∏è\n",
    "\n",
    "One survey company may predict wrong.\n",
    "\n",
    "But if you combine 100 different surveys, the combined result (average) is much more accurate.\n",
    "This is ensemble learning.\n",
    "\n",
    "### 2. Bagging (a type of ensemble)\n",
    "\n",
    "Bagging = ‚Äútrain the same model (like many decision trees), but each on different random samples of the data.‚Äù\n",
    "\n",
    "Each tree is trained on slightly different data ‚Üí so they don‚Äôt all make the same mistakes.\n",
    "\n",
    "Then we combine (majority vote for classification).\n",
    "\n",
    "üëâ Example:\n",
    "You want to know if a new restaurant is good üç¥.\n",
    "\n",
    "You ask 10 different food bloggers.\n",
    "\n",
    "Each blogger randomly tries a few items (not the full menu).\n",
    "\n",
    "Then you take their combined opinion.\n",
    "That‚Äôs bagging.\n",
    "\n",
    "‚úÖ Difference:\n",
    "\n",
    "Ensemble Learning = big umbrella (combine models).\n",
    "\n",
    "Bagging = one specific technique (build models on random subsets & combine).\n",
    "\n",
    "## üåü Part 2: Random Forest vs Decision Tree\n",
    "### 1. Decision Tree üå≥\n",
    "\n",
    "A single if-else flowchart.\n",
    "\n",
    "Simple, interpretable.\n",
    "\n",
    "But can overfit (memorize training data).\n",
    "\n",
    "üëâ Example:\n",
    "One doctor decides if you‚Äôre sick:\n",
    "\n",
    "‚ÄúIf fever > 100 ‚Üí sick, else not sick.‚Äù\n",
    "\n",
    "Works, but depends entirely on that one doctor‚Äôs judgment.\n",
    "\n",
    "### 2. Random Forest üå≤üå≤üå≤üå≤\n",
    "\n",
    "A collection of many decision trees (built using bagging).\n",
    "\n",
    "Each tree sees random data & random features.\n",
    "\n",
    "Final prediction = majority vote.\n",
    "\n",
    "üëâ Example:\n",
    "Instead of one doctor, you ask 100 doctors.\n",
    "\n",
    "Each doctor looks at a slightly different set of symptoms (data/features).\n",
    "\n",
    "Then they vote.\n",
    "\n",
    "The group decision is more reliable than one doctor.\n",
    "\n",
    "### ‚úÖ Difference:\n",
    "\n",
    "Decision Tree\tRandom Forest\n",
    "Single model\tMany trees (ensemble)\n",
    "High chance of overfitting\tLess overfitting\n",
    "Easy to interpret\tHarder to interpret\n",
    "Fast training\tSlower (many trees)\n",
    "Example: 1 doctor‚Äôs opinion\tExample: 100 doctors voting\n",
    "\n",
    "### üìå Key Insight:\n",
    "\n",
    "Decision Tree = simple but risky (overfit).\n",
    "\n",
    "Random Forest = safer, more stable, more accurate because of bagging + ensemble power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dbbd9ee-8a6e-4c8c-97af-d7d7ff8dbbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.7988826815642458\n",
      "Random Forest Accuracy: 0.8156424581005587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rx/bfj9nybj02jf94xlvrjnp9nr0000gn/T/ipykernel_1961/2462120677.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[\"age\"].fillna(X[\"age\"].median(), inplace=True)\n",
      "/var/folders/rx/bfj9nybj02jf94xlvrjnp9nr0000gn/T/ipykernel_1961/2462120677.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[\"age\"].fillna(X[\"age\"].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = sns.load_dataset(\"titanic\")\n",
    "\n",
    "# Select features and target\n",
    "X = titanic[[\"pclass\", \"age\", \"sex\", \"fare\"]]\n",
    "y = titanic[\"survived\"]\n",
    "\n",
    "# Handle missing values\n",
    "X[\"age\"].fillna(X[\"age\"].median(), inplace=True)\n",
    "\n",
    "# Convert categorical (sex) ‚Üí numeric\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# üå≥ Single Decision Tree\n",
    "tree_model = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "tree_model.fit(X_train, y_train)\n",
    "tree_preds = tree_model.predict(X_test)\n",
    "\n",
    "# üå≤ Random Forest\n",
    "forest_model = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=42)\n",
    "forest_model.fit(X_train, y_train)\n",
    "forest_preds = forest_model.predict(X_test)\n",
    "\n",
    "# Compare accuracy\n",
    "tree_acc = accuracy_score(y_test, tree_preds)\n",
    "forest_acc = accuracy_score(y_test, forest_preds)\n",
    "\n",
    "print(\"Decision Tree Accuracy:\", tree_acc)\n",
    "print(\"Random Forest Accuracy:\", forest_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ecfc28-e222-4362-87ed-c09c4ceb4449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
